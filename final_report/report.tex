\documentclass[transmag]{IEEEtran}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsfonts,amssymb,amsmath}
\usepackage{hyperref}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{titlesec}
\usepackage{bbold}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{lastpage}
\usepackage{lipsum}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
%\markboth{$>$ REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER $<$}
%{$>$ REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER $<$}
\begin{document}



\title{Predicting Ethereum Price Change Using News Articles}

\author{\IEEEauthorblockN{Ashutosh Bansal}
\textit{19323385}
\and
\IEEEauthorblockN{Ming Jun Lim}
\textit{21337483}
\and
\IEEEauthorblockN{Markus Scully}
\IEEEauthorblockA{
\textit{16321114}}}




\maketitle
\thispagestyle{plain}
\pagestyle{plain}


%\section{INTRODUCTION}
\section{Introduction}
% explain the problem and why it is interesting
\IEEEPARstart{T}{he} cryptocurrency market have witnessed an explosion in the recent years with Bitcoin\footnote{\ Bitcoin is the first well known cryptocurrency started in the late 2000s} hitting lots of new time high and many investors moving to this trading space \cite{huy2019predicting}. With the internet constantly evolving, social networking sites such as Facebook, Twitter, Instagram and etc have become a means of communication between people. There have been researches in text sentiment analysis to predict price changes for Bitcoin \cite{huy2019predicting}\cite{sattarov2020forecasting} as Bitcoin is considered a dominant cryptocurrency. However many other cryptocurrency have been developed and have been in demand such as Ethereum \footnote{\ Ethereum is a programmable blockchain and cryptocurrency described as Ether}. The project will be tackling Ethereum market price by estimating if the future price increases or decreases based on text processing sourced from the web such as social networking sites and forums. The problem is converted into a classfication problem by comparing the future market price with current market price instead of a regression problem of estimating the future market price. Can news or posts of Ethereum articles really affect the market price?

The input to our algorithm is text sourced web scrapers on social networking sites. We then compare several machine learning methods such as k-Nearest Neighbours (kNN), logistic regression and Support Vector Machine (SVM) to output a predicted label, +1 if price increased or -1 if price decreased.

\section{Dataset and Features}
The dataset consists of Ethereum related articles and class labels gathered from Twitter, Reddit and Binance\footnote{\ Binance is one of the largest cryptocurrency exchange} over the last four years. Data from Twitter and Reddit are merged with a similar column names: \emph{date} \emph{content} and \emph{popularity}.

\subsection{Data Source}

\subsection{Twitter}
Twitter provides an official API for accessing tweets but access to this requires an approval process \cite{twitter_api_docs}. Instead, the open-source Python library snscrape \cite{snscrape_git} was used to collect 240 tweets for each day in the range January 2017 to October 2021, filtered by search for the word "Ethereum". The popularity content for each tweet was calculated by summing together the number of retweets, likes and replies it received. This popularity value was then normalised by dividing it by the highest popularity value of all collected tweets.

\subsection{Reddit}
Reddit is divided into smaller sub-communities relating to specific themes called "subreddits". The subreddits "EtherMining", "Cryptocurrency", "Cryptocurrencies", "CryptoMarkets", "EthTrader", "Ethereum", and "Bitcoin" were selected as source of data because of their relevancy. Comments posted on submission on each of these subreddits were scraped using the library psaw which itself pulls data from service Pushshift. The popularity value of each comment was determined by its "karma" value, a measure of how much positive feedback it has received from other users of the site. Up to 240 comments were scraped per subreddit per day, for a maximum of 1680 per day if that many comments had actually been posted.

% Reddit
% preprocess

% Binance
\subsection{Binance}
Binance provides a set of API \cite{binance_web_socket}, WebSocket endpoints available to collect live data and old historical data \cite{binance_public_data}. Monthly trade data of ETHUSDT\footnote{\ ETHUSDT is a symbol for Ethereum and USDT pair, 1 USDT is 1 USD.} symbol is collected from August, 2017 to October, 2021 with a total of 657,446,190 data points. These data points are processed by calculating the average price by days and discretizing the continuous values into +1 and -1 class labels by comparing it with the previous day resulting into 1,537 data points.

\subsection{Feature Extraction}

Several pre-processing steps are applied. The first step is tokenization, splitting input character sequences into tokens. The endings of the tokens are chopped of by stemming. The tokens are pruned using a set of stop words as they contribute little to no information such as words "is", "the", "a", etc. Tokens appearing frequently and rarely were also pruned through the \verb|min_df| and \verb|max_df| parameters in tokenization, this step was very important to reduce the feature size. The algorithm uses Natural Language Toolkit \verb|nltk| library providing stemming functions such as \verb|PorterStemmer| and a list of stopwords \verb|nltk.corpus.stopwords.words("english")|. Some general pre-processing on text data from \cite{sattarov2020forecasting} were applied onto our dataset consisting of:
\begin{itemize}
  \item Converting all words to lower case.
  \item Removing words containing numbers.
  \item Removing symbols from words ("\#","\@@").
  \item Removing words containing non\-english characters.
\end{itemize}

These sequence of tokens are mapped to a feature vector using the bag of words model, the model consists of hyper-parameter n-grams and cross validation for 1-gram single token and 2-gram pairs of token are evaluated in section ???. The n-gram hyper-parameter enables model to preserve some word ordering but feature vector size can grow quickly.

\section{Methods}

\subsection{SVM}
SVM 

\subsection{}

\section{Experiments}

\section{Results}

\section{Discussion}

\section{Summary}








\bibliographystyle{plain}
\bibliography{bibliography.bib}


%\newpage\null\thispagestyle{empty}\newpage

\end{document}